# generated by datamodel-codegen:
#   filename:  catalog.yml
#   timestamp: 2024-04-17T07:51:34+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, RootModel
from dat_core.pydantic_models import DatDocumentStream
from dat_core.pydantic_models.base import EnumWithStr

class ChunkingStrategy(EnumWithStr):
    split_by_html_header = 'split_by_html_header'
    split_by_html_section = 'split_by_html_section'
    split_by_character = 'split_by_character'
    split_code = 'split_code'
    markdown_header_text_splitter = 'markdown_header_text_splitter'
    recursively_split_json = 'recursively_split_json'
    recursiverly_split_by_character = 'recursiverly_split_by_character'
    semantic_chunking = 'semantic_chunking'
    split_by_tokens = 'split_by_tokens'


class HeadersToSplitOnItem(RootModel):
    root: List[str] = Field(
        min_items=2,
        max_items=2
    )


class SplitterConfig(BaseModel):
    headers_to_split_on: Optional[List[HeadersToSplitOnItem]] = Field(
        None,
        description='list of tuples of headers we want to track mapped to (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4, h5, h6',
        min_items=1,
    )


class Advanced(BaseModel):
    chunking_strategy: Optional[ChunkingStrategy] = 'recursiverly_split_by_character'
    splitter_config: Optional[SplitterConfig] = None


class Crawler(DatDocumentStream):
    name: Optional[str] = 'url_crawler'
    namespace: Optional[str] = Field(
        None, description='namespace the data is associated with'
    )
    advanced: Optional[Advanced] = Field(
        None, description='Additional optional settings'
    )


class WebCrawlerCatalog(BaseModel):
    document_streams: List[Crawler]
